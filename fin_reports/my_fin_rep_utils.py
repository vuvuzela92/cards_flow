import asyncio
import aiohttp
import pandas as pd
import logging
from datetime import datetime, timedelta
import json
import traceback
from dotenv import load_dotenv
import os
import asyncpg




# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ API —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞ tokens.json
def load_api_tokens():
    with open('tokens.json', encoding= 'utf-8') as f:
        tokens = json.load(f)
        return tokens
# next_request_time –±—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º—è, –∫–æ–≥–¥–∞ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞
# –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è —Ä–µ–π—Ç-–ª–∏–º–∏—Ç–∞ Wildberries API
next_request_time = {}
# request_lock –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ next_request_time
# –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≥–æ–Ω–æ–∫ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
# –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ—Ä—É—Ç–∏–Ω –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ–ø—ã—Ç–∞—é—Ç—Å—è –æ–±–Ω–æ–≤–∏—Ç—å –≤—Ä–µ–º—è, —ç—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –æ—à–∏–±–∫–∞–º
request_lock = asyncio.Lock()
# semaphore –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API
# semaphore = asyncio.Semaphore(8)  # –º–∞–∫—Å–∏–º—É–º 8 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

async def get_fin_reports_async(account: str, api_token: str, date_from: str, date_to: str, request_lock: asyncio.Lock, next_request_time: dict):
    """–ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –æ—Ç—á—ë—Ç—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏ retry."""
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å: –∫–æ–≥–¥–∞ –º–æ–∂–Ω–æ —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å –ø–æ –∞–∫–∫–∞—É–Ω—Ç—É

    async with asyncio.Semaphore(8) as semaphore:
        print(f"üîç {account} | –ó–∞–ø—Ä–æ—Å –∑–∞ {date_from} ‚Äì {date_to}")

    url = "https://statistics-api.wildberries.ru/api/v5/supplier/reportDetailByPeriod"
    headers = {"Authorization": api_token}
    #  —É–∫–∞–∑–∞—Ç–µ–ª—å –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.
    rrdid = 0
    # —Å–ø–∏—Å–æ–∫, –≤ –∫–æ—Ç–æ—Ä—ã–π –±—É–¥—É—Ç —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å—Å—è –≤—Å–µ DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏.
    all_data = []

    timeout = aiohttp.ClientTimeout(total=120, connect=30, sock_read=60, sock_connect=15)
    # –°–æ–∑–¥–∞—ë—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è HTTP-—Å–µ—Å—Å–∏—è. –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –±—É–¥—É—Ç –∏–¥—Ç–∏ —á–µ—Ä–µ–∑ –Ω–µ—ë.
    async with aiohttp.ClientSession(timeout=timeout) as session:
        attempt = 0
        max_attempts = 20
        last_rrdid = None # –∑–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è (–µ—Å–ª–∏ rrdid –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è)

        while attempt < max_attempts:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–æ–≥–¥–∞ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —ç—Ç–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞
                async with request_lock:
                    now = datetime.now()
                    # –ï—Å–ª–∏ –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ –Ω–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –µ–≥–æ –≤ —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è
                    last_call = next_request_time.get(account, now)
                    # –ï—Å–ª–∏ —Å–µ–π—á–∞—Å –º–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, –∂–¥—ë–º
                    if now < last_call:
                        # –í—ã—á–∏—Å–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ –ø–æ–¥–æ–∂–¥–∞—Ç—å
                        wait = (last_call - now).total_seconds()
                        logging.warning(f"[{account}] –ñ–¥—ë–º {wait:.1f} —Å–µ–∫ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞")
                        await asyncio.sleep(wait)
                params = {
                    "dateFrom": date_from,
                    "dateTo": date_to,
                    "limit": 50000,
                    "rrdid": rrdid,
                }
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º GET-–∑–∞–ø—Ä–æ—Å –∫ API Wildberries
                async with session.get(url, headers=headers, params=params) as response:
                    # ‚úÖ –ï—Å–ª–∏ –ø–æ–ª—É—á–∞–µ–º 429 –æ—à–∏–±–∫—É, –∑–Ω–∞—á–∏—Ç –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤.
                    if response.status == 429:
                        logging.warning(f"[429] {account} | –õ–∏–º–∏—Ç. –ñ–¥—ë–º 65 —Å–µ–∫...")
                        async with request_lock:
                            next_request_time[account] = datetime.now() + timedelta(seconds=65)
                        attempt += 1
                        await asyncio.sleep(1)
                        continue

                    # ‚úÖ –ï—Å–ª–∏ –≤—ã—Ö–æ–¥–∏—Ç 400 –æ—à–∏–±–∫–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ —Å–≤—è–∑–∞–Ω–∞ –ª–∏ –æ–Ω–∞ —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ—à–∏–±–∫–∞–º–∏.
                    # –µ—Å–ª–∏ –Ω–µ—Ç, —Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                    if response.status == 400:
                        text = await response.text()
                        logging.warning(f"[400] {account} | Bad Request: {text[:500]}")

                        # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –≤ —Ç–æ–∫–µ–Ω–µ –∏–ª–∏ rrdid ‚Äî –Ω–µ–ª—å–∑—è –ø–æ–≤—Ç–æ—Ä—è—Ç—å
                        critical = ["invalid", "token", "rrdid", "malformed", "parameter"]
                        if any(word in text.lower() for word in critical):
                            logging.error(f"üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ 400: {text[:200]}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                            break

                        # –ï—Å–ª–∏ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ ‚Äî –ø—Ä–æ–±—É–µ–º –ø–æ–≤—Ç–æ—Ä–∏—Ç—å
                        logging.info(f"[400] –ü–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –∑–∞–¥–µ—Ä–∂–∫—É...")
                        async with request_lock:
                            next_request_time[account] = datetime.now() + timedelta(seconds=65)
                        attempt += 1
                        await asyncio.sleep(2 * attempt)
                        continue

                    # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—É—Å—ã
                    if response.status != 200:
                        text = await response.text()
                        logging.error(f"[{response.status}] {account}: {text[:500]}")
                        break

                    raw_text = await response.text()
                    if not raw_text.strip():
                        logging.warning(f"üì° {account}: –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                        break

                    try:
                        data = json.loads(raw_text)
                        if isinstance(data, list) and not data:
                            break
                        print(f"–ü–æ–ª—É—á–µ–Ω—ã —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ ({len(data)} —Å—Ç—Ä–æ–∫):", data[:1])  # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                    except json.JSONDecodeError as je:
                        logging.error(f"üí• JSON Error {account}: {je}")
                        logging.error(f"Raw (first 1000): {raw_text[:1000]}")
                        break

                    if not data or (isinstance(data, dict) and data.get("errors")):
                        logging.info(f"üü¢ {account}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ {date_from}")
                        break

                    df = pd.DataFrame(data)
                    if df.empty:
                        logging.info(f"üü° {account}: –ü—É—Å—Ç–æ–π DataFrame")
                        break
                    print("–°–æ–∑–¥–∞–Ω DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:", df.columns.tolist())
                    print("–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:", df.iloc[0].to_dict())

                    df["account"] = account
                    df['dlv_prc'] = df['dlv_prc'].astype(str)
                    all_data.append(df)

                    logging.info(f"üì• {account} | +{len(df)} —Å—Ç—Ä–æ–∫ | {date_from}‚Äì{date_to}")
                    print(f'–ü–æ {account} –ø–æ–ª—É—á–∞–µ–º rrdid->{df["rrd_id"].iloc[-1]}')

                    next_rrdid = df["rrd_id"].iloc[-1]
                    if next_rrdid == rrdid or next_rrdid == last_rrdid:
                        break
                    if next_rrdid <= rrdid:
                        break
                    rrdid = next_rrdid
                    last_rrdid = next_rrdid

                    delay = 60
                    if len(df) >= 25000:
                        delay = 70

                    async with request_lock:
                        next_request_time[account] = datetime.now() + timedelta(seconds=delay)

            except aiohttp.ClientPayloadError as e:
                logging.warning(f"üì° [Payload] {account}: {e}. –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                attempt += 1
                await asyncio.sleep(5 * attempt)
                continue

            except asyncio.TimeoutError:
                logging.warning(f"‚è∞ Timeout {account}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                attempt += 1
                await asyncio.sleep(5 * attempt)
                continue

            except (aiohttp.ClientConnectorError, ConnectionResetError) as e:
                logging.warning(f"üîå –°–µ—Ç—å {account}: {type(e).__name__}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                attempt += 1
                await asyncio.sleep(5 * attempt)
                continue

            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ {account}: {type(e).__name__}: {e}")
                logging.error(f"TRACEBACK:\n{traceback.format_exc()}")
                break

        result = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç
        date_cols_list = ['date_from', 'date_to', 'create_dt', 'fix_tariff_date_from',
                        'fix_tariff_date_to', 'rr_dt', 'order_dt', 'sale_dt'] 

        for col in date_cols_list:
            if col in result.columns:
                result[col] = pd.to_datetime(result[col], errors='coerce')
                if pd.api.types.is_datetime64tz_dtype(result[col]):
                    result[col] = result[col].dt.tz_localize(None)
                if col in ['order_dt', 'sale_dt']:
                    result[col] = pd.Series(
                        [x.to_pydatetime() if pd.notna(x) else None for x in result[col]],
                        index=result.index,
                        dtype='object'
                    )
                else:
                    result[col] = result[col].apply(
                        lambda x: x.date() if pd.notna(x) else None
                    )

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö —Ç–∏–ø–æ–≤
        for col in result.columns:
            if result[col].dtype in ['int64', 'int32', 'int16', 'int8']:
                result[col] = pd.Series(
                    [None if pd.isna(x) else int(x) for x in result[col]],
                    index=result.index,
                    dtype='object'
                )
            elif result[col].dtype in ['float64', 'float32', 'float16']:
                result[col] = pd.Series(
                    [None if pd.isna(x) else float(x) for x in result[col]],
                    index=result.index,
                    dtype='object'
                )
            elif result[col].dtype == 'bool':
                result[col] = pd.Series(
                    [None if pd.isna(x) else bool(x) for x in result[col]],
                    index=result.index,
                    dtype='object'
                )

        # –Ø–≤–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è BOOLEAN-–∫–æ–ª–æ–Ω–æ–∫ (–¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏, –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –æ—à–∏–±–∫–∏)
        bool_cols = ['is_kgvp_v2', 'srv_dbs', 'is_legal_entity']
        for col in bool_cols:
            if col in result.columns:
                result[col] = pd.Series(
                    [None if pd.isna(x) else bool(x) for x in result[col]],
                    index=result.index,
                    dtype='object'
                )

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (VARCHAR/TEXT)
        text_cols = [
            'currency_name', 'suppliercontract_code', 'dlv_prc', 'subject_name', 'brand_name',
            'sa_name', 'ts_name', 'barcode', 'doc_type_name', 'office_name', 'supplier_oper_name',
            'gi_box_type_name', 'payment_processing', 'acquiring_bank', 'ppvz_office_name',
            'ppvz_supplier_name', 'ppvz_inn', 'declaration_number', 'bonus_type_name',
            'sticker_id', 'site_country', 'rebill_logistic_org', 'kiz', 'trbx_id', 'account'
        ]
        for col in text_cols:
            if col in result.columns:
                result[col] = pd.Series(
                    [None if pd.isna(x) else str(x) for x in result[col]],
                    index=result.index,
                    dtype='object'
                )

        logging.info(f"‚úÖ {account} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(result)} —Å—Ç—Ä–æ–∫ –∑–∞ {date_from}‚Äì{date_to}")
        print(result.info())
        print(result.dtypes)
        return result

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ª–æ–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã
table_creation_lock = asyncio.Lock()

async def create_insert_table_db_async(df: pd.DataFrame, table_name: str, columns_type: dict, key_columns: tuple):
    load_dotenv()

    conn = None
    try:
        conn = await asyncpg.connect(
            user=os.getenv('USER_2'),
            password=os.getenv('PASSWORD_2'),
            database=os.getenv('NAME_2'),
            host=os.getenv('HOST_2'),
            port=os.getenv('PORT_2'),
            timeout=10
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        valid_types = ['INTEGER', 'BIGINT', 'SMALLINT', 'NUMERIC', 'DATE', 'TIMESTAMP', 'BOOLEAN', 'TEXT', 'VARCHAR']
        for col, dtype in columns_type.items():
            if not dtype.strip():
                raise ValueError(f"–ü—É—Å—Ç–æ–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ {col}")
            base_type = dtype.split('(')[0].strip().upper()
            if base_type not in valid_types:
                raise ValueError(f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ {col}: {dtype}")
            if '(' in dtype and base_type not in ['NUMERIC', 'VARCHAR']:
                raise ValueError(f"–°–∫–æ–±–∫–∏ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã –¥–ª—è —Ç–∏–ø–∞ {base_type} –≤ –∫–æ–ª–æ–Ω–∫–µ {col}: {dtype}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
        missing_cols = set(columns_type.keys()) - set(df.columns)
        for col in missing_cols:
            df[col] = None
            logging.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–æ–Ω–∫–∞ {col} —Å None")
        extra_cols = set(df.columns) - set(columns_type.keys())
        if extra_cols:
            logging.warning(f"–õ–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ DataFrame: {extra_cols}, –æ–Ω–∏ –±—É–¥—É—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã")

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ SQL –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫
        columns_definition = ", ".join([f"{col} {dtype}" for col, dtype in columns_type.items()])
        # —Å–æ–∑–¥–∞–µ—Ç SQL-–≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è (UNIQUE constraint) –≤ —Ç–∞–±–ª–∏—Ü–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
        unique_constraint = f"CONSTRAINT unique_{table_name} UNIQUE ({', '.join(key_columns)})" if key_columns else ""

        # –û—Ç–ª–∞–¥–∫–∞: –≤—ã–≤–æ–¥ SQL-–∑–∞–ø—Ä–æ—Å–∞
        create_table_query = f"""
            DO $$
            BEGIN
                IF NOT EXISTS (SELECT FROM pg_tables WHERE tablename = '{table_name}') THEN
                    CREATE TABLE {table_name} (
                        {columns_definition}{', ' + unique_constraint if unique_constraint else ''}
                    );
                END IF;
            END$$;
        """



        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        async with table_creation_lock:
            await conn.execute(create_table_query)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
        columns = list(columns_type.keys())  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ columns_type
        records = [tuple(None if pd.isna(x) else x for x in row) for row in df[columns].to_records(index=False)]

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ UPSERT-–∑–∞–ø—Ä–æ—Å–∞
        updates = ', '.join([f"{col}=EXCLUDED.{col}" for col in columns if col not in key_columns])
        query = f"""
            INSERT INTO {table_name} ({', '.join(columns)})
            VALUES ({', '.join([f'${i+1}' for i in range(len(columns))])})
            ON CONFLICT ON CONSTRAINT unique_{table_name}
            DO UPDATE SET {updates}
        """
        
        await conn.executemany(query, records)
        logging.info(f"–£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –≤ {table_name}")
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –ë–î: {str(e)}")
        raise
    finally:
        if conn:
            await conn.close()



async def fetch_all_data(accounts_tokens, num_weeks=1):
    today = datetime.today()
    weekday = today.weekday()
    current_sunday = today - timedelta(days=(weekday + 1) % 7)
    processed_weeks = 0

    for week in range(num_weeks):
        current_monday = current_sunday - timedelta(days=6)
        date_from = current_monday.strftime('%Y-%m-%d')
        date_to = current_sunday.strftime('%Y-%m-%d')

        logging.info(f"üìÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–¥–µ–ª—é {week + 1}/{num_weeks}: {date_from} ‚Äì {date_to}")

        tasks = [
            get_fin_reports_async(account, token, date_from, date_to, request_lock, next_request_time)
            for account, token in accounts_tokens.items()
        ]

        weekly_results = await asyncio.gather(*tasks, return_exceptions=True)

        table_name = 'fin_reports_full'
        columns_type = {
            "realizationreport_id": "INTEGER",
            "date_from": "DATE",
            "date_to": "DATE",
            "create_dt": "DATE",
            "currency_name": "TEXT",
            "suppliercontract_code": "TEXT",
            "rrd_id": "BIGINT",
            "gi_id": "BIGINT",
            "dlv_prc": "TEXT",
            "fix_tariff_date_from": "DATE",
            "fix_tariff_date_to": "DATE",
            "subject_name": "TEXT",
            "nm_id": "BIGINT",
            "brand_name": "TEXT",
            "sa_name": "TEXT",
            "ts_name": "TEXT",
            "barcode": "TEXT",
            "doc_type_name": "TEXT",
            "quantity": "INTEGER",
            "retail_price": "NUMERIC(12,2)",
            "retail_amount": "NUMERIC(12,2)",
            "sale_percent": "SMALLINT",
            "commission_percent": "NUMERIC(5,2)",
            "office_name": "TEXT",
            "supplier_oper_name": "TEXT",
            "order_dt": "TIMESTAMP",
            "sale_dt": "TIMESTAMP",
            "rr_dt": "DATE",
            "shk_id": "BIGINT",
            "retail_price_withdisc_rub": "NUMERIC(12,2)",
            "delivery_amount": "INTEGER",
            "return_amount": "INTEGER",
            "delivery_rub": "NUMERIC(12,2)",
            "gi_box_type_name": "TEXT",
            "product_discount_for_report": "NUMERIC(5,2)",
            "supplier_promo": "NUMERIC(12,2)",
            "ppvz_spp_prc": "NUMERIC(5,2)",
            "ppvz_kvw_prc_base": "NUMERIC(5,2)",
            "ppvz_kvw_prc": "NUMERIC(5,2)",
            "sup_rating_prc_up": "NUMERIC(5,2)",
            "is_kgvp_v2": "BOOLEAN",
            "ppvz_sales_commission": "NUMERIC(12,2)",
            "ppvz_for_pay": "NUMERIC(12,2)",
            "ppvz_reward": "NUMERIC(12,2)",
            "acquiring_fee": "NUMERIC(12,2)",
            "acquiring_percent": "NUMERIC(5,2)",
            "payment_processing": "TEXT",
            "acquiring_bank": "TEXT",
            "ppvz_vw": "NUMERIC(12,2)",
            "ppvz_vw_nds": "NUMERIC(12,2)",
            "ppvz_office_name": "TEXT",
            "ppvz_office_id": "INTEGER",
            "ppvz_supplier_id": "INTEGER",
            "ppvz_supplier_name": "TEXT",
            "ppvz_inn": "TEXT",
            "declaration_number": "TEXT",
            "bonus_type_name": "TEXT",
            "sticker_id": "TEXT",
            "site_country": "TEXT",
            "srv_dbs": "BOOLEAN",
            "penalty": "NUMERIC(12,2)",
            "additional_payment": "NUMERIC(12,2)",
            "rebill_logistic_cost": "NUMERIC(12,2)",
            "rebill_logistic_org": "TEXT",
            "storage_fee": "NUMERIC(12,2)",
            "deduction": "NUMERIC(12,2)",
            "acceptance": "NUMERIC(12,2)",
            "assembly_id": "BIGINT",
            "srid": "TEXT",
            "report_type": "SMALLINT",
            "is_legal_entity": "BOOLEAN",
            "trbx_id": "TEXT",
            "installment_cofinancing_amount": "NUMERIC(12,2)",
            "wibes_wb_discount_percent": "SMALLINT",
            "cashback_amount": "NUMERIC(12,2)",
            "cashback_discount": "NUMERIC(5,2)",
            "account": "VARCHAR(50)",
            }
        key_columns = ['realizationreport_id', 'rrd_id', 'srid']

        save_tasks = []
        for result in weekly_results:
            if isinstance(result, Exception):
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –Ω–µ–¥–µ–ª–∏ {week + 1}: {result}")
                continue
            if isinstance(result, pd.DataFrame) and not result.empty:
                save_tasks.append(
                    create_insert_table_db_async(result, table_name, columns_type, key_columns)
                )

        if save_tasks:
            await asyncio.gather(*save_tasks, return_exceptions=True)
        processed_weeks += 1
        logging.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–¥–µ–ª–∏ {week + 1}/{num_weeks}")

        current_sunday -= timedelta(days=7)

    logging.info(f"‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_weeks}/{num_weeks} –Ω–µ–¥–µ–ª—å")